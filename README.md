### Deep Learning using Neural Attention and Multi-Tasking

Applied to the SEMEVAL research task [2016 ABSA](http://alt.qcri.org/semeval2016/task5/#)

The task was removed from SEMEVAL-2017, but still looking to slowly update/structure codebase from Tensorflow 0.10. This is partial code base of prior implementation.

Focus on multi-class/label tasks and used target phrases for attention.

##### Refrences
* Neural Attention & RNNs
	* Neural machine translation by jointly learning to align and translate
	* Effective Approaches to Attention-based Neural Machine Translation https://arxiv.org/abs/1508.04025
	*  Neural and Syntactic Models of Entity-Attribute Relationship for Aspect-based Sentiment Analysis

* Multi Tasking
	* Multitask Learning Author: Rich Caruana http://link.springer.com/article/10.1023/A:1007379606734
	* https://www.ncbi.nlm.nih.gov/pubmed/21992749
* Convolutions for NLP
	* Convolutional Neural Networks for Sentence Classification https://arxiv.org/abs/1408.5882
	* [WildML - UNDERSTANDING CONVOLUTIONAL NEURAL NETWORKS FOR NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/) 	
	* https://github.com/yoonkim/CNN_sentence
	* INSIGHT-1 at SemEval-2016 Task 5: Deep Learning for Multilingual Aspect-based Sentiment Analysis	


